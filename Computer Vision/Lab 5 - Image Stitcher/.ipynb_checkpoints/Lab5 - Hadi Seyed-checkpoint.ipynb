{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Image Stitching for Panorams\n",
    "\n",
    "### What to Submit\n",
    "Submit this iPython Notebook--containing all your code for the programming exercises below--on Canvas.\n",
    "\n",
    "### Programming Exercise\n",
    "\n",
    "For this assignment, you will be writing a program that creates an image panorama from 2 or more images.  In general this technique should be applicable to any number of photographs.  The approach described below will work well for collective fields of up to 90 or even 120&deg;, but won't produce ideal results for large fields of view approaching or surpassing 180&deg;.  For large fields of view, cylindrical or spherical projection is required.\n",
    "\n",
    "When we construct a panorama, we assume that all of the photographs were taken from the exact same location and that the images are related by pure rotation (no translation of the camera).  The easiest way to create the panorama is to project all of the photos onto a plane.  One photo must be selected (either manually or by your program) to be the base photo.  The other photos are aligned to this base photo by identifying a homography (a planar warp specified by 4 pairs of source/destination points) relating each pair.  Each of the other images is appropriately warped and composited onto the plane (the base image doesn’t need to be warped).\n",
    "\n",
    "In describing what you need to do, there will be a running example using the three photos below:\n",
    "\n",
    "\n",
    "<div style=\"width:100%;text-align:center;\"><img src=\"Images/example1.png\" width=100%></div>\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A: Find Interest Points/Descriptors in each Input Image\n",
    "We will be using OpenCV for this project, which you should already have installed. Although the SIFT algorithm is patented and used to require a separate install, it is now included in the newer versions of OpenCV. A good tutorial on how to use SIFT features in OpenCV is found [here](https://docs.opencv.org/trunk/da/df5/tutorial_py_sift_intro.html).  The first step to registering or aligning two images is to identify locations in each image that are distinctive or stand out.  The `sift.detectAndCompute()` routine produces both these interest points and their corresponding SIFT descriptors.  The first step of producing a panorama is to load all of the relevant images and find the interest points and their descriptors.\n",
    "\n",
    "See the circles on each image below indicating the sift keypoints that were found (note that we downsampled the images to 600 x 600 pixels before extracting SIFT). The circles are scaled according to the scale at which each keypoint was detected at and the radius indicates the dominate gradient magnitude. Output a similar visual in your code.\n",
    "\n",
    "<div style=\"width:100%;text-align:center;\"><img src=\"Images/example2.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Matching Features\n",
    "\n",
    "Next, given the features present in each image, you need to match the features so as to determine corresponding points between adjacent/overlapping images.  [This page](https://docs.opencv.org/trunk/dc/dc3/tutorial_py_matcher.html) provides details to do feature matching using `cv2.BFMatcher()`, analogous to the approach proposed by David Lowe in his original implementation.  Be aware that the resulting match is one directional.  You want to find putative pairs--pairs of points which are each other’s best match (e.g. there might be 3 points in image I1 for which a point q in image I2 are the best match, only one of these could be the best matching point p in I1 for that point q in I2).  In this part you need to compute the set of putative matches between each pair of images.\n",
    "\n",
    "Look at the pairs of images and the lines showing the estimated matches (putative matches are green lines, one way matches are cyan or blue). Output a similar visual in your code (the images can be side by side, it doesn't have to be vertical).\n",
    "\n",
    "<div style=\"width:100%;text-align:center;\"><img src=\"Images/example3.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C: Estimating Pairwise Homographies using RANSAC\n",
    "\n",
    "Use the RANSAC algorithm ([Szeliski](http://szeliski.org/Book/), Ch 8.1.4), estimate the homography between each pair of images.  You will need to decide whether you’re going to manually specify the base image or determine in programmatically.  Along with identifying the base image, you need to figure out the order in which you will composite the other images to the base.\n",
    "\n",
    "You will need 4 pairs of best-match points to estimate a homography for each composited image. Below you will find a visualization of the RANSAC estimated homographies.  Images 1, 2, and 3 have dots that are red, green and blue respectively (sorry the dots are a little small), representing the putative pairs.  You can see where the homographies line up very well and in a few places (the middle vertically) they line up slightly less well.\n",
    "\n",
    "For this section, you may use `cv2.findHomography()` or use the code from Lab 4 to generate the homography matrix. However, you must implement RANSAC yourself. Additionally, you may simply take the best homography of those randomly sampled, but as a possible improvement, you may implement a least squares optimization over the largest consesus set.\n",
    "\n",
    "<div style=\"width:100%;text-align:center;\"><img src=\"Images/example4.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part D: Creating the Mosaic\n",
    "\n",
    "Begin with the base image and warp the remaining images (using the estimated homographies) to composite them onto the base image. You may use `cv2.warpPerspective()` for the backwards warping (Note: You may need to multiply your homography by a translation matrix keep it in the frame. You can then translate it back to put it on the correct part of the mosaic.).\n",
    "\n",
    "For the ongoing campus example, here are the resulting warped images composited.\n",
    "\n",
    "<div style=\"width:100%;text-align:center;\"><img src=\"Images/example5.png\" width=75%></div>\n",
    "\n",
    "And, then with a very simple (but not ideal) compositing operation using the median of the mapped pixels.\n",
    "\n",
    "<div style=\"width:100%;text-align:center;\"><img src=\"Images/example6.png\" width=75%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A: Find Interest Points/Descriptors (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def find_interest_points_and_descriptors(image_path):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Resize the images to the desired size\n",
    "    image = cv2.resize(image,(500,900))\n",
    "    \n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Initialize the SIFT detector\n",
    "    sift = cv2.SIFT_create()\n",
    "\n",
    "    # Detect keypoints and compute descriptors\n",
    "    keypoints, descriptors = sift.detectAndCompute(gray_image, None)\n",
    "\n",
    "    # Draw keypoints on the image\n",
    "    image_with_keypoints = cv2.drawKeypoints(image, keypoints, None)\n",
    "\n",
    "    return keypoints, descriptors, image_with_keypoints\n",
    "\n",
    "# List of input image paths\n",
    "image_paths = ['campus1.jpg', 'campus2.jpg', 'campus3.jpg']\n",
    "\n",
    "for image_path in image_paths:\n",
    "    keypoints, descriptors, image_with_keypoints = find_interest_points_and_descriptors(image_path)\n",
    "\n",
    "    # Display or save the image with keypoints\n",
    "    cv2.imshow('Image with Keypoints', image_with_keypoints)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Matching Features (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def find_and_match_features(campus1, campus2):\n",
    "    # Load the two images\n",
    "    img1 = cv2.imread(campus1, cv2.IMREAD_GRAYSCALE)\n",
    "    img2 = cv2.imread(campus2, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    # Resize the images to the desired size\n",
    "    img1 = cv2.resize(img1, (400, 900))\n",
    "    img2 = cv2.resize(img2, (400, 900))\n",
    "\n",
    "    # Initialize the SIFT detector\n",
    "    sift = cv2.SIFT_create()\n",
    "\n",
    "    # Find keypoints and descriptors for both images\n",
    "    keypoints1, descriptors1 = sift.detectAndCompute(img1, None)\n",
    "    keypoints2, descriptors2 = sift.detectAndCompute(img2, None)\n",
    "\n",
    "    # Initialize a Brute-Force Matcher\n",
    "    bf = cv2.BFMatcher()\n",
    "\n",
    "    # Find best matches using KNN (k-nearest neighbors)\n",
    "    matches = bf.knnMatch(descriptors1, descriptors2, k=2)\n",
    "\n",
    "    # Apply a ratio test to select good matches\n",
    "    good_matches = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < 0.75 * n.distance:\n",
    "            good_matches.append(m)\n",
    "\n",
    "    # Draw the matches\n",
    "    img_matches = cv2.drawMatches(img1, keypoints1, img2, keypoints2, good_matches, None)\n",
    "\n",
    "    return img_matches\n",
    "\n",
    "# List of image pairs for feature matching\n",
    "image_pairs = [('campus1.jpg', 'campus3.jpg'), ('campus2.jpg', 'campus3.jpg')]\n",
    "\n",
    "# Create an empty canvas to display the matched images\n",
    "output_canvas = None\n",
    "\n",
    "for campus1, campus2 in image_pairs:\n",
    "    img_matches = find_and_match_features(campus1, campus2)\n",
    "\n",
    "    # Stack the matched images side by side\n",
    "    if output_canvas is None:\n",
    "        output_canvas = img_matches\n",
    "    else:\n",
    "        output_canvas = np.hstack((output_canvas, img_matches))\n",
    "\n",
    "# Display or save the output canvas with matches\n",
    "cv2.imshow('Feature Matches', output_canvas)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C: Estimating Pairwise Homographies using RANSAC (30 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def estimate_homography_RANSAC(src_pts, dst_pts, ransac_threshold=5.0):\n",
    "    # Use RANSAC to estimate the homography matrix\n",
    "    M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, ransac_threshold)\n",
    "    return M, mask\n",
    "\n",
    "# List of image pairs for feature matching\n",
    "image_pairs = [('campus1.jpg', 'campus3.jpg'), ('campus2.jpg', 'campus3.jpg')]\n",
    "\n",
    "# Create an empty canvas to display the matched images\n",
    "output_canvas = None\n",
    "\n",
    "# List of images and their corresponding keypoints and descriptors\n",
    "image_paths = ['campus1.jpg', 'campus2.jpg', 'campus3.jpg']\n",
    "keypoints_list = []\n",
    "descriptors_list = []\n",
    "\n",
    "# Populate keypoints and descriptors for each image\n",
    "for image_path in image_paths:\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    image = cv2.resize(image, (400, 900))\n",
    "    sift = cv2.SIFT_create()\n",
    "    keypoints, descriptors = sift.detectAndCompute(image, None)\n",
    "    keypoints_list.append(keypoints)\n",
    "    descriptors_list.append(descriptors)\n",
    "    \n",
    "\n",
    "# Determine the base image (e.g., the image with the most keypoints)\n",
    "base_image_idx = 2 #np.argmax([len(keypoints) for keypoints in keypoints_list])\n",
    "\n",
    "# Initialize the list to store pairwise homographies\n",
    "homographies = []\n",
    "\n",
    "# Loop through the images to estimate pairwise homographies\n",
    "for idx, (keypoints1, descriptors1) in enumerate(zip(keypoints_list, descriptors_list)):\n",
    "    print(idx)\n",
    "    \n",
    "    if idx != base_image_idx:\n",
    "        # Match keypoints between the base image and the current image\n",
    "        bf = cv2.BFMatcher()\n",
    "        matches = bf.knnMatch(descriptors_list[base_image_idx], descriptors1, k=2)\n",
    "\n",
    "        # Apply a ratio test to select good matches\n",
    "        good_matches = []\n",
    "        for m, n in matches:\n",
    "            if m.distance < 0.75 * n.distance:\n",
    "                good_matches.append(m)\n",
    "\n",
    "        # Extract matched keypoints and their corresponding points\n",
    "        src_pts = np.float32([keypoints_list[base_image_idx][m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "        dst_pts = np.float32([keypoints1[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "\n",
    "        # Estimate homography using RANSAC\n",
    "        homography, mask = estimate_homography_RANSAC(src_pts, dst_pts, ransac_threshold=5.0)\n",
    "        homographies.append(homography)\n",
    "\n",
    "        # Warp the non-base images onto the base image\n",
    "        image = cv2.imread(image_paths[idx])\n",
    "        image = cv2.resize(image, (400, 900))\n",
    "        warped_image = cv2.warpPerspective(image, homography, (image.shape[1], image.shape[0]))\n",
    "\n",
    "        # Blend the warped image with the base image\n",
    "        if output_canvas is None:\n",
    "            output_canvas = warped_image\n",
    "        else:\n",
    "            output_canvas = cv2.addWeighted(output_canvas, 0.5, warped_image, 0.5, 0)\n",
    "\n",
    "# Display or save the final panorama\n",
    "cv2.imshow('Panorama', output_canvas)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part D: Creating the Mosaic (30 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# List of image paths and their corresponding homographies\n",
    "image_paths = ['campus1.jpg', 'campus2.jpg', 'campus3.jpg']\n",
    "homographies = []  # List of homography matrices obtained in the previous step\n",
    "\n",
    "# Read the base image (the image that doesn't need warping)\n",
    "base_image = cv2.imread(image_paths[0])\n",
    "image = cv2.resize(image, (400, 900))\n",
    "\n",
    "# Initialize the final mosaic as the base image\n",
    "mosaic = base_image.copy()\n",
    "\n",
    "# Loop through the other images and warp them onto the mosaic\n",
    "for i in range(1, len(image_paths)):\n",
    "    image = cv2.imread(image_paths[i])\n",
    "    \n",
    "    # Calculate the homography for the current image\n",
    "    img1 = base_image  # Use the base image as the first image\n",
    "    img2 = image\n",
    "    \n",
    "    # Detect keypoints and compute descriptors\n",
    "    orb = cv2.ORB_create()\n",
    "    keypoints1, descriptors1 = orb.detectAndCompute(img1, None)\n",
    "    keypoints2, descriptors2 = orb.detectAndCompute(img2, None)\n",
    "\n",
    "    # Create a Brute-Force Matcher\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "    # Match descriptors\n",
    "    matches = bf.match(descriptors1, descriptors2)\n",
    "\n",
    "    # Sort the matches by distance\n",
    "    matches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "    # Get the corresponding points from the matches\n",
    "    points1 = np.float32([keypoints1[match.queryIdx].pt for match in matches]).reshape(-1, 1, 2)\n",
    "    points2 = np.float32([keypoints2[match.trainIdx].pt for match in matches]).reshape(-1, 1, 2)\n",
    "\n",
    "    # Compute the homography matrix using RANSAC\n",
    "    homography, _ = cv2.findHomography(points1, points2, cv2.RANSAC, 5.0)\n",
    "    homographies.append(homography)\n",
    "\n",
    "    # Add a translation to keep the warped image in frame\n",
    "    translation_matrix = np.zeros((3, 3), dtype=np.float64)\n",
    "    translation_matrix[0, 2] = -homography[0, 2]  # Translate in the x-direction\n",
    "    translation_matrix[1, 2] = -homography[1, 2]  # Translate in the y-direction\n",
    "    translation_matrix[2, 2] = 1  # Set the bottom-right element to 1\n",
    "\n",
    "    # Apply the translation\n",
    "    warped_image = cv2.warpPerspective(image, translation_matrix, (image.shape[1], image.shape[0]))\n",
    "        \n",
    "    # Simple compositing by taking the median of overlapping pixels\n",
    "    mosaic = cv2.addWeighted(mosaic, 0.5, warped_image, 0.5, 0)\n",
    "    \n",
    "# Display or save the final mosaic\n",
    "cv2.imshow('Mosaic', mosaic)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
