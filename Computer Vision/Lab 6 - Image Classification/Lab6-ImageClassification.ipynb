{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6 - Intro to Image Classification with Convolutional Neural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background - Image Classification\n",
    "\n",
    "For this project you will be introduced to the basics of Convolutional Neural Networks (CNNs) and the PyTorch framework.  Deep learning with CNNs can be very computationally expensive and runs fastest with GPU support.  If you do not have access to GPUs on your local machine, you can use some from Google using their [colab tool](https://colab.research.google.com).  Colab runs exactly like jupyter notebooks and you can directly upload your .ipynb file. If you have a Mac machine that has a M1 or M2 processor, you can also use MPS acceleration for a slight speed up.\n",
    "\n",
    "Image classification is the task of taking an image and labeling it as a category.  CNNs have been a leading method for image classification since it dominated the [ImageNet competition in 2012](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf).\n",
    "\n",
    "The instructions for the lab are contained in this Jupyter notebook. **You may choose to complete the lab in this notebook, or you may choose to complete the lab in standard Python using the .py template file.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A: Dataset\n",
    "\n",
    "You will be performing classification on the [Stanford Cars Dataset](https://www.kaggle.com/datasets/jessicali9530/stanford-cars-dataset), which consists of 16,185 images of 196 classes of cars.  The dataset is split 50-50 into a training set and testing set.  You can download the images and the needed annotation files from this [Google Drive folder](https://drive.google.com/drive/folders/1GLHQAt3KNN_3eIETkinzlY5q9HG44Blx?usp=sharing).  Each set is around 1 GB of data so please **<span style=\"color:red\">DO NOT</span>** include the files when you upload your notebook--**just turn in the .ipynb or .py file**.  Assume the notebooks will be run with the images in folders labeled `cars_train` and `cars_test` like so:\n",
    "\n",
    "```\n",
    ".\n",
    "+--proj6-image-classification.ipynb\n",
    "+--cars_train\n",
    "|  +--00001.jpg\n",
    "|  +--00002.jpg\n",
    "|  +--...\n",
    "+--cars_test\n",
    "|  +--00001.jpg\n",
    "|  +--00002.jpg\n",
    "|  +--...\n",
    "+--test_annos.json\n",
    "+--train_annos.json\n",
    "```\n",
    "\n",
    "The first step in any neural network method is to make sure you can read in the data.  Since there will be a lot of images for this project, it is possible not all of them will fit into memory.  This is a common problem in CNNs and PyTorch has provided a pattern so as to only have the images you need in memory at any given time.  They provide a class called `DataLoader` that acts as an iterable object.  To use `DataLoader`, you will need to implement a subclass of PyTorch's `Dataset` class.  To do you so will need to create a class that inherits from `Dataset` and implements the methods `__getitem__` and `__len__`.  An example is given below and PyTorch provides a [tutorial here](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html). You can also reference the Custom Image Loader found in [this tutorial](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, filename):\n",
    "        super(MyDataset, self).__init__()\n",
    "        # TODO: implement what happens when someone calls: dataset = MyDataset()\n",
    "        # Pull in relevant file information, images lists, etc.\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # TODO: implement what happens when someone calls dataset[idx]\n",
    "        # Return an image and it's associated label at location idx \n",
    "    \n",
    "    def __len__(self):\n",
    "        # TODO: implement what happens when someone calls len(dataset)\n",
    "        # Determine the number of images in the dataset\n",
    "\n",
    "training_dataset = MyDataset(\"train_annos.json\")\n",
    "loader = DataLoader(training_dataset,batch_size=1)\n",
    "for im, label in loader:\n",
    "      print(im.shape,label)\n",
    "      # More stuff we will implement a later in the lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide you with two files `test_annos.json` and `train_annos.json`.  These files contain a dictionary mapping image name to the class label the image belongs to.  You can use these files in you Dataset class in order to provide the ground truth labels.  For part A, you will need to implement a dataset class.\n",
    "\n",
    "**Note:** Both the images and the labels are 1-indexed. You can load the images however you choose, but the labels must be 0-indexed to work with Pytorch's loss functions. Make sure to account for this in your Dataset class.\n",
    "\n",
    "**Note:** Make sure the images and labels that are returned are both PyTorch tensors. Specifically, the images should be returned as Tensors that rearranged to be shape [channels, rows, cols] and should be values between 0 and 1. To guarantee this, I recommend using `read_image` from `torchvision.io` and dividing by 255.\n",
    "\n",
    "**Note:** Some of the images in the dataset are grayscale while most are RGB. To prevent issues later, make sure to load all images as RGB. If you use `read_image`, you can use `ImageReadMode.RGB` to gaurantee that all images have 3 channels.\n",
    "\n",
    "**Note:** Python has a [json library](https://docs.python.org/3/library/json.html) that you can use to turn json files in to Python dictionaries. You can look up some simple tutorials online."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B: Neural Network Architecture\n",
    "\n",
    "The main backbone for deep learning is the actual neural network architecture.  For image classification, this will consist of some combination of `Conv2d` layers with activations--usually `ReLU`--with intermitted downsampling--usually done using `MaxPool2d`--followed by a few Linear layers.  The input to the network should be an image with shape `(batch_size, channels, image_height, image_width)`(e.g. an single image with dimensions 224x224 would be `(1, 3, 224, 224)`) and output a vector of shape `(num_classes,)` where the largest value's index in the output vector indicates the class label.  \n",
    "\n",
    "While we built our own network in the mini-lab, for this lab we will used one of Pytorch's pretrained networks. This has the benefit of already having learned features from training on an ImageNet classification problem. To pull in this pretrained network, we use the following line of code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "print(list(model.__dict__[\"_modules\"].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ResNet18 is a common baseline network that uses convolution layers, batch normalization, layers of residual blocks, and a fully connected layer at the end. The different layers are listed above. However, because the pretrained network was trained on ImageNet, the last layer is designed to predict 1000 classes, not 196 like in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.fc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to use the same architecture as the ResNet, but replace the last fully connected layer with a new fully connected layer that goes from 512 input features to 196 output features.\n",
    "\n",
    "PyTorch provides a nice framework for making a neural network architecture.  A network is typically made as a class that inherits from PyTorch's `Module` class and implments the `forward` method.  A network might take the form of the example below. PyTorch also provides a simple Neural Network [tutorial here](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html), the Training a Classifier tutorial is especially helpful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "\n",
    "class MyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNetwork, self).__init__()\n",
    "        ref_model = torchvision.models.resnet18(pretrained=True)\n",
    "        self.conv1 = ref_model.conv1\n",
    "        self.bn1 = ref_model.bn1\n",
    "        self.relu = ref_model.relu\n",
    "        # TODO: Continue network setup here\n",
    "        # TODO: Replace fc layer with your own linear layer that outputs 196 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        # TODO: Continue feeding output through all layers of the network\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take all the pretrained layers from ResNet18, but then define your own last fully connected layer. Then write the appropriate forward pass function.\n",
    "\n",
    "**Note:** ResNet was trained with images that are normalized according to the ImageNet color averages. This means you may want to include an appropriate normalization in your Dataset class if you did not already. This can easily be done with Pytorch's transform objects.\n",
    "\n",
    "```Python\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part C: Training\n",
    "\n",
    "Now that you can access your data and you have a network architecture setup, its time to put things together and start training.  Training requires two major components: 1) the loss function and 2) the optimizer.  The loss function is a comparison between your results and the ground truth data.  The optimizer is what takes the results of the loss function and backpropagates the error to the network weights in an attempt to decrease the loss.  The most common loss functions used for classification is [Cross Entropy](https://pytorch.org/docs/stable/nn.html#crossentropyloss) while the most commonly used optimizer function is [Adam](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam).  \n",
    "\n",
    "A basic training step might take the following form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = MyDataset(\"train_annos.json\")\n",
    "train_loader = DataLoader(training_dataset,batch_size=1,shuffle=True)\n",
    "\n",
    "model = MyNetwork()\n",
    "\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for images,labels in train_loader:\n",
    "    optimizer.zero_grad()\n",
    "    outs = model(inputs)\n",
    "    loss = loss_func(outs, labels)  # loss_func would be an instance of a torch.nn.CrossEntropyLoss class\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For deliverables on this section, modify the above code to store the loss after every few passes (use `loss.item()` go get the value without the tensor info). Then display a plot of the value of the loss over time.  If things are working, the loss should be decreasing.\n",
    "\n",
    "You may also choose to run your training loop multiple times. Each run of the training loop is called an **epoch**.\n",
    "\n",
    "**Note**: This step could take several hours so you will want to look into being able to save your model to a file and load it up again.\n",
    "\n",
    "**Note**: Mac computers sometimes create .DS_STORE files inside of directories. If your training or testing loop breaks at a random time, it may be trying to load a hidden .DS_STORE file that it thinks is an image. Since they are hidden in the the Finder window, you will need to use the terminal to navigate to the data folder and use command `rm .DS_STORE`.\n",
    "\n",
    "**Tip**: If you want a good sense of how long your code is going to take to run, use the `tqdm` class to time the FOR loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part D: Testing\n",
    "\n",
    "One of the goals of deep learning is to make a model that generalizes to data it has never seen (e.g. new images of cars).  For this part, you will test your generalizability by running the model on a dataset it has not yet seen during training.  To do so, you will also need to put the model into  you will need to make sure you are not calculating any of the gradients by using `torch.no_grad` in a with statement. You will aslo need to put the network into evaluation mode:\n",
    "```Python\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # enter testing code here\n",
    "```\n",
    "\n",
    "To put the network back into training mode, call `model.train()`.\n",
    "\n",
    "You will compare your predictions with the ground truth value for value.  The output of your network, however, will be a vector of length 196 (the number of possible classes for the cars dataset) with the **largest** value representing the guessed class.  You'll need to extract the guessed class number and compare it with the ground truth number for all images in the test dataset and calculate the overall accuracy.  Print out the overall accuacy your model got.\n",
    "\n",
    "While high test accuracy is not the only goal in this lab, most students are able to get above 50% in their testing accuracy. If you are unable to reach this level of accuracy, it may indicate an error in your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # enter testing code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading\n",
    "Points for this assigment will be assigned as follows (100 points total):\n",
    "* [30 pts] Making a Dataset class\n",
    "* [10 pts] Setting up you architecture\n",
    "* [30 pts] Training your model and plotting training loss\n",
    "* [20 pts] Displaying the overall accuracy of your model\n",
    "\n",
    "The last 10 points are earned through completing a subset of the following explorations:\n",
    "* [10 pts] Increase the batch size of your training network. To do so, you will need to garauntee that all images in a batch are the same spatial resolution or it will not run. Thus, you will need to add random crop data augmentation, but make sure the crop is not too small, otherwise you might miss parts of the car. Describe the effects an increased batch_size had on training.\n",
    "* [10 pts] Modify your code to include GPU acceleration using .cuda calls in the appropriate places. If you have an M1 or M2 processor Mac, you can modify your code to have MPS acceleration. Describe the speed-ups to the training loop after implementing acceleration.\n",
    "* [10 pts] Enhance your dataloader to include reflection data augmentation (i.e. double the size of your training data by taking the mirror image across the y-axis). **DO NOT** do reflection augmentation across the x-axis (we don't care to detect cars when they are upside down!). You may also add other augmentations. Describe what effects the augmentation had on testing accuracy.\n",
    "* [20 pts] Generate a confusion matrix of the 196 categories. A confusion matrix shows how often a specific category is guessed as each other category (you can search for example plots online). For example, the 11th row and 34th column in the matrix should tell you how many times category 11 images were guessed to be category 34 images. Thus, a perfect predictor on the test set would have nonzero values only along the diagnol. Once you generated the confusion matrix, you may simply plot it as a grayscale image (with interpolation turned off).\n",
    "* [10 pts] Analyze the effect of learning rates on the accuracy of the network. Describe what you found and give supporting plots.\n",
    "* [10 pts] Analyze the effect of epochs on the accuracy of the network. Describe what you found and give supporting plots.\n",
    "* [10 pts] Analyze the effect of batch_size and varying optimizers on the accuracy of the network. Describe what you found and give supporting plots. \n",
    "* [10 pts] Analyze the effect of varying optimizers on the accuracy of the network. Describe what you found and give supporting plots. A list of optimizers in Pytorch can be found [here](https://pytorch.org/docs/stable/optim.html).\n",
    "* [10 pts] Analyze the effect of different pretrained networks on the accuracy of the network. Describe what you found and give supporting plots. A list of pretrained networks in Pytorch can be found [here](https://pytorch.org/docs/stable/torchvision/models.html).\n",
    "\n",
    "You may earn up to 20 points extra credit for additional explorations you complete.\n",
    "\n",
    "An additional 15 points of extra credit will be given to the individual with the highest test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please describe which explorations you completed in the markdown cell below (or in the comments on Canvas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
